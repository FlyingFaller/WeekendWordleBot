WeekendWordleBot is a work-in-progress bot to """optimally""" solve the NYT Wordle game I decided to make for fun one (three) weekened. It only has knowledge of the 14,855 valid guess words (kinda), not the much small solution set (which doesn't exist publically anymore). It's based in Shannon entropy/information theory and there are many explinations out there for similar bots see the excellent videos by 3b1b. 

Explination: 

There are many kinds of Wordle bots that have been designed, but these can be catagorized broadly into two groups: ones that use real time computations and cheap heuristics to gauge the quality of a guess, and ones that precompute the search space ahead of time at great expense. The latter can outperforrm the heuristic-based bots and can even acheive provably-optimal performance as demonstrated in the paper *An Exact and Interpretable Solution to Wordle* by Bertsimas and Paskov which leveraged exact dynamic programming to complete the search. However, heuristic-based approaches can acheive remarkably similar results to their exhaustive counterparts while keeping the computations near real-time. According to Bertsimas and Paskov, "The current form of Wordle — with 6 rounds, 5 letter words, and a guess and solution space of sizes 10,657 and 2,315, respectively — took days to solve via an efficient C++ implementation of the algorithm, parallelized across a 64-core computer". While the advantage of this kind of approach is a "one-and-done" computation producing a perfectly optimal solution, the result is very brittle. What happens if more valid guesses are added or the valid answers are changed? The computation would have to be redone at great expense. Since the publishing of this paper, the game has undergone many changes. First, the valid guess space was expanded signifigantly to 14,855 words. Then, in January 2022, the New York Times (NYT) aquired the property and took over creative control of what answer was chosen each day. While the viral popularity of Wordle has long since died out, it isn't suprising few new bots have been released since the NYT aquisition. There is no longer a reliable list of answer words to use as the choices are at the whims of a NYT edditor who, inconviently, does not publish all the future answers they plan to pick. This creates several problems for bot development: 
1. Using the pre-NYT answer list creates signifigant risk that the bot will be unable to solve for out-of-list words
2. Using the valid guess set for safety hurts performance by dramatically expanding the number of words which must be elimintated
3. Without knowledge of what words can or cannot be answers it is impossible to make a provably-optimal bot and the immense computation is less valuable

In my quest to build the best bot I could for the current version of Wordle, I had to tackle many of these challenges and what follows is a brief description of my own solution. I knew I wanted to use close to real-time computations where possible rather than spending days and days on mediocre hardware precomputing a decision tree that could become invalid in a weeks time. This locked me in to using heuristic-pruning of the search space at some point. While many heuristic approaches use heuristics to both score and prune, for example many simple bots try to maximize gained entropy over a small guess horizon, I found that using heuristic scoring always led to fundemental mismatch between what a bot should optimize for and what it actually was optimizing for. 

For entropy scoring, potential next guesses have to be compared on equal terms. You would not learn anything useful about the relative quaility of two words when one is scored on entropy gained after one guess and the other is scored on entropy gained after two guesses. Futhermore, there is a maximial limit to the amount of entropy that can be gaiend over the course of a game. Thus it is not useful to try and maximize entropy gained over the span of a game as many, many candidate guesses will achieve this maximal entropy. If a middle ground is chosen, say entropy after three guesses, you are still not optimizing for the fastest average solution. Instead you are optimizing for entropy gained after 3 guesses or in otherwords likely hood of having the solution after three guesses. Fastest average solution and likelyhood of knowing the solution after N guesses are related but not equivalents. 

A better way of scoring words is by simply doing a deep and complete search of the tree that follows from a candidate guess and averaging the number of moves needed to solve for all possible answers. A lower average number of moves necessarly means a faster average solve which is exactly what we want. The issue now is that we are falling back on exhaustive tree searches which are infeasbile. The best solution I've found is to incooperate both ideas. We can use a greedy, depth-one entropy score to reliably find only the most promissing candidate guesses and then evaluate only their resultant trees. At each move in the tree we can prune again by the same entropy strategy. This reduces the search tree by many orders of magnitude and, when combined with numerous implementation optimizations, makes an exhaustive search (of the pruned tree) possible. While the greedy entropy filter does not garuntee optimality, when combined with a tree search I can achieve results very similar to the non-pruned exahastive decision trees. The logic is that while the optimal guess may not be the word that provides the most single round entropy gain, it will almost always be near the top of the list. If you cast a wide enough net, and don't prune too many branches, you are almost certain to explore down the optimal branch. The extent to which you can reduce the answer space before a bot works to reduce it further greatly impacts how good the solutions will be, so as long as the NYT does not publish their answer set this bot is about as optimal as any other approach can be. These statements are made based on vibes alone. 

